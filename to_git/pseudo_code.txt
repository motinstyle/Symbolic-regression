SYMBOLIC REGRESSION ALGORITHM

1. INITIALIZATION
   1.1. Load configuration parameters from parameters.py
   1.2. Create diverse initial population:
       - Ensure each function appears at least once as root
       - Create one tree with a constant node if CONST_OPT is False
       - Fill remaining population with unique random trees
       - Initialize variable counts (var_count) for all nodes
   1.3. Initialize set of unique expressions

2. EVOLUTION LOOP (for num_epochs iterations)
   2.1. Track unique expressions in current population
   2.2. For each model in population:
       - Create copy for evaluation
   
   2.3. If not first epoch:
       a) CROSSOVER PHASE
          - For each model:
              * Try up to 10 times to create unique offspring
              * Select parent2 using tournament selection between two random models
              * Perform crossover with selected parent
              * Update variable counts and tree depth
              * Add to evaluation list if expression is unique
       
       b) EVALUATION PHASE (after crossover)
          - Evaluate all models from crossover phase:
              * Calculate forward loss (RMSE)
              * Add domain penalty if applicable
              * Calculate inverse error if REQUIRES_GRAD is true
       
       c) SELECTION PHASE (intermediate)
          - Select best models after crossover for mutation using specified method
          - If SELECTION_METHOD is "NSGA-II":
              * Perform non-dominated sorting into fronts
              * Calculate crowding distance within fronts
              * Select models based on front rank and crowding distance
          - If SELECTION_METHOD is "top_k":
              * Sort by total error
              * Select top k models
       
       d) MUTATION PHASE
          - For each selected model:
              * Try up to 10 times to create unique mutant
              * Apply mutation with probability MUTATION_PROB
              * Apply constant mutation if CONST_OPT is False with probability CONST_MUTATION_PROB
              * Update variable counts and tree depth
              * Add to population if expression is unique

   2.4. If CONST_OPT and epoch % CONST_OPT_FREQUENCY == 0:
        - Optimize constants in population
   
   2.5. Evaluate all models in final population
   
   2.6. Track and update best model:
        - Update if current model has lower error
        - Store error history
        - Check for early stopping if min_loss < 0.0001

3. MODEL EVALUATION
   3.1. Calculate error components:
       - Forward loss (RMSE between predictions and targets)
       - Domain penalty (for function domain violations)
       - Inverse error (if REQUIRES_GRAD is true)
   
   3.2. Total error calculation:
       error = forward_loss + domain_penalty + inv_loss

4. TREE OPERATIONS

   4.1. Build Random Tree:
       - Input: num_vars, max_depth, functions, requires_grad, allow_constants
       - Recursively build tree up to max_depth
       - Calculate var_count (number of variables) for each node
       - Return variable node or constant node at leaves
       - Ensure unique expressions in population

   4.2. Mutation:
       - Randomly select node
       - Replace with compatible function (same parity)
       - Maintain tree structure
       - Update variable counts for affected nodes
       - Try multiple times for unique expression

   4.3. Crossover:
       - Use tournament selection to choose second parent based on multiple criteria
       - Select random nodes from two parents
       - Exchange subtrees
       - Update variable counts and recalculate tree depth
       - Validate resulting tree
       - Ensure unique expression

   4.4. Constant Optimization:
       - Periodically optimize constant values
       - Only if CONST_OPT is true
       - Use gradient-based optimization

   4.5. Tree Depth Calculation:
       - Nodes with zero variables (var_count = 0) are treated as leaf nodes
       - Subtrees containing only constants do not contribute to tree depth
       - Depth calculation uses var_count to determine effective tree structure

5. TOURNAMENT SELECTION
   5.1. Compare two models based on multiple criteria:
       - Error value (RMSE)
       - Tree complexity (number of nodes)
       - Domain loss penalties
       - Forward loss and inverse loss
   5.2. If one model dominates on all criteria, select it
   5.3. Otherwise, select model with higher crowding distance (more unique)

6. OUTPUT
   6.1. Return best model found during evolution
   6.2. Plot error history
   6.3. Display final statistics:
       - Best error achieved
       - Number of unique expressions found
       - Final mathematical expression 